{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7047707-008f-451f-bede-c88593a50a27",
   "metadata": {},
   "source": [
    "## Test with a small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf455b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = 'data/checkpoints/0906_214036/model_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc2b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "from utils.unique import unique\n",
    "import model.loss as module_loss\n",
    "import model.model as module_arch\n",
    "import model.metric as module_metric\n",
    "from parse_config import ConfigParser\n",
    "import data_loader.data_loaders as module_data\n",
    "from utils.span2json import span2json\n",
    "from utils.conll2span import conll2span\n",
    "from utils.correcting_labels import fix_labels\n",
    "\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0baa04c-5085-49d1-bf95-61907c720449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.correcting_labels import fix_labels, remove_incorrect_tag\n",
    "def get_dict_prediction(tokens, preds, attention_mask, ids2tag):\n",
    "    temp_preds=[]\n",
    "    for index in range(len(preds)):    \n",
    "        if attention_mask[index]==1:\n",
    "            Ptag = ids2tag.get(preds[index].item())\n",
    "            temp_preds.append(Ptag)\n",
    "            \n",
    "    temp_preds = remove_incorrect_tag(temp_preds, \"BIOES\")\n",
    "    temp_preds = fix_labels(temp_preds, \"BIOES\")    \n",
    "    temp_preds = conll2span(temp_preds)\n",
    "    temp_preds = span2json(tokens, temp_preds)   \n",
    "    return temp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614cbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser(description='PyTorch Template')\n",
    "args.add_argument('-c', '--config', default=None, type=str, help='config file path (default: None)')\n",
    "args.add_argument('-r', '--resume', default=f\"{resume}\", type=str, help='path to latest checkpoint (default: None)')\n",
    "args.add_argument('-d', '--device', default=None, type=str, help='indices of GPUs to enable (default: all)')\n",
    "args.add_argument('-f', '--file', default=None, type=str, help='Error')\n",
    "\n",
    "config = ConfigParser.from_args(args)\n",
    "logger = config.get_logger('test')\n",
    "\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "test_data_loader = data_loader.get_test()\n",
    "\n",
    "# build model architecturea\n",
    "model = config.init_obj('arch', module_arch)\n",
    "# logger.info(model)\n",
    "\n",
    "# get function handles of loss and metrics\n",
    "criterion = getattr(module_loss, config['loss'])\n",
    "metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "logger.info('Loading checkpoint: {} ...'.format(config.resume))\n",
    "checkpoint = torch.load(config.resume)\n",
    "state_dict = checkpoint['state_dict']\n",
    "if config['n_gpu'] > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(state_dict)\n",
    "layers_train = config._config['trainer']['layers_train']\n",
    "\n",
    "# prepare model for testing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "total_metrics = torch.zeros(len(metric_fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce78f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, instance in tqdm(enumerate(test_data_loader)):\n",
    "        input_ids = torch.tensor(instance['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(instance['attention_mask']).to(device)\n",
    "        batch_size = input_ids.shape[0]\n",
    "        output = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = 0\n",
    "        nested_lm_conll_ids = {l:None for l in range(len(layers_train))}\n",
    "        for index, layer in enumerate(layers_train):\n",
    "            temp_nested_lm_conll_ids = torch.tensor(instance['nested_lm_conll_ids'][layer])\n",
    "            temp_nested_lm_conll_ids = temp_nested_lm_conll_ids.to(device)\n",
    "            nested_lm_conll_ids[index]=temp_nested_lm_conll_ids\n",
    "            loss+=criterion(output[index], temp_nested_lm_conll_ids)\n",
    "            \n",
    "        total_loss += loss.item() * batch_size\n",
    "        predictions = {x:[] for x in range(batch_size)}\n",
    "        lm_entities = {x:[] for x in range(batch_size)}\n",
    "        for sent_ids in range(batch_size):\n",
    "            for layer in range(len(output)):\n",
    "                predictions[sent_ids].append(output[layer][sent_ids].argmax(axis=0))\n",
    "                lm_entities[sent_ids].append(nested_lm_conll_ids[layer][sent_ids])\n",
    "        for sent_ids in range(batch_size):\n",
    "            tokens = instance['lm_tokens'][sent_ids]\n",
    "            tokens = [w for w in tokens if w!=PAD]\n",
    "            preds = []\n",
    "            for index in range(len(layers_train)):\n",
    "                preds+=get_dict_prediction(\n",
    "                        tokens, \n",
    "                        predictions[sent_ids][index], \n",
    "                        attention_mask[sent_ids], \n",
    "                        data_loader.ids2tag)\n",
    "            entities_labels = []\n",
    "            for index in range(len(layers_train)):\n",
    "                entities_labels+=get_dict_prediction(\n",
    "                    tokens, \n",
    "                    lm_entities[sent_ids][index], \n",
    "                    attention_mask[sent_ids], \n",
    "                    data_loader.ids2tag)\n",
    "            results.append({\n",
    "                'sentence_id': instance['sentence_id'][sent_ids],\n",
    "                'tokens': tokens,\n",
    "                'entities': entities_labels,\n",
    "                'predictions':preds})\n",
    "            for i, metric in enumerate(metric_fns):\n",
    "                total_metrics[i] += metric( \n",
    "                        output, nested_lm_conll_ids, attention_mask, \n",
    "                        data_loader.boundary_type, info=False, ids2tag=data_loader.ids2tag\n",
    "            ) * batch_size     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114f1b2-8ece-46d3-82ec-1d7fb36e65b6",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03f91d-57d1-4389-ac13-eb6b6be2fb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ids, item in enumerate(results[:2]):\n",
    "    print('\\n',\"|\".join(item[\"tokens\"]),'\\n')\n",
    "    sid = item['sentence_id']\n",
    "    entities=item['entities']\n",
    "    predictions=item['predictions']\n",
    "    \n",
    "    print(\"Predictions\")\n",
    "    for pred in predictions:\n",
    "        tag = \"/\" if pred in entities else \"X\"\n",
    "        out = [f\"SID:{sid}, Index{pred['span']}\", tag, ''.join(pred['text']), pred['entity_type']]\n",
    "        print(out)\n",
    "    \n",
    "    print(\"Not answer\")\n",
    "    for en in entities:\n",
    "        if en not in predictions:\n",
    "            out = [f\"SID:{sid}, Index{en['span']}\", \"X\", \"\".join(en['text']), en['entity_type']]\n",
    "            print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can input both BIESO and BIO\n",
    "from model.eval import ClassEvaluator\n",
    "results_eval, conll_results = ClassEvaluator()(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842504b-3412-4d27-b327-326ba8b24ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace] *",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
